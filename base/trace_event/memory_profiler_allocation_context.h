// Copyright 2015 The Chromium Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef BASE_TRACE_EVENT_MEMORY_PROFILER_ALLOCATION_CONTEXT_H_
#define BASE_TRACE_EVENT_MEMORY_PROFILER_ALLOCATION_CONTEXT_H_

#include <map>
#include <string>
#include <vector>

#include "base/atomicops.h"
#include "base/base_export.h"
#include "base/trace_event/trace_event_impl.h"

namespace base {
namespace trace_event {

class TraceEventMemoryOverhead;

// When heap profiling is enabled, tracing keeps track of the allocation
// context for each allocation intercepted. It is generated by the
// |AllocationContextTracker| which keeps stacks of context in TLS.
// The tracker is initialized lazily.

using StackFrame = const char*;

// A simple stack of |StackFrame| that unlike |std::stack| allows iterating
// the stack and guards for underflow.
class BASE_EXPORT AllocationStack {
 public:
  // Incrementing the iterator iterates up the stack, from bottom (least recent
  // call) to top (most recent call).
  using ConstIterator = std::vector<StackFrame>::const_iterator;

  AllocationStack();
  ~AllocationStack();

  inline ConstIterator bottom() const { return stack_.begin(); }
  inline ConstIterator top() const { return stack_.end(); }

  inline void push(StackFrame frame) {
    // Impose a limit on the height to verify that every push is popped, because
    // in practice the pseudo stack never grows higher than ~20 frames.
    DCHECK_LT(stack_.size(), 128u);
    stack_.push_back(frame);
  }

  inline void pop(StackFrame frame) {
    if (stack_.empty())
      return;

    // Assert that pushes and pops are nested correctly.
    // This DCHECK can be hit if some TRACE_EVENT macro is unbalanced
    // (a TRACE_EVENT_END* call without a corresponding TRACE_EVENT_BEGIN).
    DCHECK_EQ(frame, stack_.back())
        << "Encountered an unmatched TRACE_EVENT_END";

    stack_.pop_back();
  }

 private:
  std::vector<StackFrame> stack_;

  DISALLOW_COPY_AND_ASSIGN(AllocationStack);
};

// The backtrace in the allocation context is a snapshot of the stack. For now,
// this is the pseudo stack where frames are created by trace event macros. In
// the future, we might add the option to use the native call stack. In that
// case, |Backtrace| and |AllocationContextTracker::GetContextSnapshot| might
// have different implementations that can be selected by a compile time flag.

// The number of stack frames stored in the backtrace is a trade off between
// memory used for tracing and accuracy. Measurements done on a prototype
// revealed that:
//
// - In 60 percent of the cases, stack depth <= 7.
// - In 87 percent of the cases, stack depth <= 9.
// - In 95 percent of the cases, stack depth <= 11.
//
// See the design doc (https://goo.gl/4s7v7b) for more details.

struct BASE_EXPORT Backtrace {
  // Unused backtrace frames are filled with nullptr frames. If the stack is
  // higher than what can be stored here, the bottom frames are stored. Based
  // on the data above, a depth of 12 captures the full stack in the vast
  // majority of the cases.
  StackFrame frames[12];
};

bool BASE_EXPORT operator==(const Backtrace& lhs, const Backtrace& rhs);

// A data structure that allows grouping a set of backtraces in a space-
// efficient manner by creating a call tree and writing it as a set of (node,
// parent) pairs. The tree nodes reference both parent and children. The parent
// is referenced by index into |frames_|. The children are referenced via a map
// of |StackFrame|s to index into |frames_|. So there is a trie for bottum-up
// lookup of a backtrace for deduplication, and a tree for compact storage in
// the trace log.
class BASE_EXPORT StackFrameDeduplicator : public ConvertableToTraceFormat {
 public:
  // A node in the call tree.
  struct FrameNode {
    FrameNode(StackFrame frame, int parent_frame_index);
    ~FrameNode();

    StackFrame frame;

    // The index of the parent stack frame in |frames_|, or -1 if there is no
    // parent frame (when it is at the bottom of the call stack).
    int parent_frame_index;

    // Indices into |frames_| of frames called from the current frame.
    std::map<StackFrame, int> children;
  };

  using ConstIterator = std::vector<FrameNode>::const_iterator;

  StackFrameDeduplicator();

  // Inserts a backtrace and returns the index of its leaf node in |frames_|.
  // Returns -1 if the backtrace is empty.
  int Insert(const Backtrace& bt);

  // Iterators over the frame nodes in the call tree.
  ConstIterator begin() const { return frames_.begin(); }
  ConstIterator end() const { return frames_.end(); }

  // Writes the |stackFrames| dictionary as defined in https://goo.gl/GerkV8 to
  // the trace log.
  void AppendAsTraceFormat(std::string* out) const override;
  void EstimateTraceMemoryOverhead(TraceEventMemoryOverhead* overhead) override;

 private:
  ~StackFrameDeduplicator() override;

  std::map<StackFrame, int> roots_;
  std::vector<FrameNode> frames_;

  DISALLOW_COPY_AND_ASSIGN(StackFrameDeduplicator);
};

// The |AllocationContext| is context metadata that is kept for every allocation
// when heap profiling is enabled. To simplify memory management for
// bookkeeping, this struct has a fixed size. All |const char*|s here
// must have static lifetime.
// TODO(ruuda): Make the default constructor private to avoid accidentally
// constructing an instance and forgetting to initialize it. Only
// |AllocationContextTracker| should be able to construct. (And tests.)
struct BASE_EXPORT AllocationContext {
  // A type ID is a number that is unique for every C++ type. A type ID is
  // stored instead of the type name to avoid inflating the binary with type
  // name strings. There is an out of band lookup table mapping IDs to the type
  // names. A value of 0 means that the type is not known.
  using TypeId = uint16_t;

  Backtrace backtrace;
  TypeId type_id;
};

bool BASE_EXPORT operator==(const AllocationContext& lhs,
                            const AllocationContext& rhs);

// The allocation context tracker keeps track of thread-local context for heap
// profiling. It includes a pseudo stack of trace events. On every allocation
// the tracker provides a snapshot of its context in the form of an
// |AllocationContext| that is to be stored together with the allocation
// details.
class BASE_EXPORT AllocationContextTracker {
 public:
  // Globally enables capturing allocation context.
  // TODO(ruuda): Should this be replaced by |EnableCapturing| in the future?
  // Or at least have something that guards agains enable -> disable -> enable?
  static void SetCaptureEnabled(bool enabled);

  // Returns whether capturing allocation context is enabled globally.
  inline static bool capture_enabled() {
    // A little lag after heap profiling is enabled or disabled is fine, it is
    // more important that the check is as cheap as possible when capturing is
    // not enabled, so do not issue a memory barrier in the fast path.
    if (subtle::NoBarrier_Load(&capture_enabled_) == 0)
        return false;

    // In the slow path, an acquire load is required to pair with the release
    // store in |SetCaptureEnabled|. This is to ensure that the TLS slot for
    // the thread-local allocation context tracker has been initialized if
    // |capture_enabled| returns true.
    return subtle::Acquire_Load(&capture_enabled_) != 0;
  }

  // Pushes a frame onto the thread-local pseudo stack.
  static void PushPseudoStackFrame(StackFrame frame);

  // Pops a frame from the thread-local pseudo stack.
  static void PopPseudoStackFrame(StackFrame frame);

  // Returns a snapshot of the current thread-local context.
  static AllocationContext GetContextSnapshot();

  ~AllocationContextTracker();

 private:
  AllocationContextTracker();

  static AllocationContextTracker* GetThreadLocalTracker();

  static subtle::Atomic32 capture_enabled_;

  // The pseudo stack where frames are |TRACE_EVENT| names.
  AllocationStack pseudo_stack_;

  DISALLOW_COPY_AND_ASSIGN(AllocationContextTracker);
};

}  // namespace trace_event
}  // namespace base

namespace BASE_HASH_NAMESPACE {

template <>
struct hash<base::trace_event::Backtrace> {
  size_t operator()(const base::trace_event::Backtrace& backtrace) const;
};

template <>
struct hash<base::trace_event::AllocationContext> {
  size_t operator()(const base::trace_event::AllocationContext& context) const;
};

}  // BASE_HASH_NAMESPACE

#endif  // BASE_TRACE_EVENT_MEMORY_PROFILER_ALLOCATION_CONTEXT_H_
